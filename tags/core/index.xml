<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Core on babifarm.com</title>
    <link>http://babifarm.com/tags/core/</link>
    <description>Recent content in Core on babifarm.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 23 Dec 2018 10:06:46 -0700</lastBuildDate>
    <atom:link href="http://babifarm.com/tags/core/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>neo4j remote access config and deploy</title>
      <link>http://babifarm.com/post/2018/12/neo4j_remote_access_config_and_deploy/</link>
      <pubDate>Sun, 23 Dec 2018 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2018/12/neo4j_remote_access_config_and_deploy/</guid>
      <description>neo4j远端访问配置及部署 1、neo4j的安装及配置 下载：neo4j-community-3.4.10-unix.tar.gz wget https://neo4j.com/artifact.php?name=neo4j-community-3.4.10-unix.tar.gz neo4j配置neo4j.conf: cat /export/soft/neo4j-community-3.4.10/conf/neo4j.conf |grep &#39;\=&#39;|grep -v &#39;#&#39; dbms.directories.import=import dbms.security.auth_enabled=false dbms.connectors.default_listen_address=0.0.0.0 dbms.connector.bolt.enabled=true dbms.connector.bolt.tls_level=OPTIONAL dbms.connector.bolt.listen_address=:7687 dbms.connector.http.enabled=true dbms.connector.http.listen_address=:7474 dbms.connector.https.enabled=true dbms.connector.https.listen_address=:7473 dbms.tx_log.rotation.retention_policy=1 days dbms.jvm.additional=-XX:+UseG1GC dbms.jvm.additional=-XX:-OmitStackTraceInFastThrow dbms.jvm.additional=-XX:+AlwaysPreTouch dbms.jvm.additional=-XX:+UnlockExperimentalVMOptions dbms.jvm.additional=-XX:+TrustFinalNonStaticFields dbms.jvm.additional=-XX:+DisableExplicitGC dbms.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048 dbms.jvm.additional=-Djdk.tls.rejectClientInitiatedRenegotiation=true dbms.windows_service_name=neo4j dbms.jvm.additional=-Dunsupported.dbms.udc.source=tarball neo4启动： /export/soft/neo4j-community-3.4.10/bin/neo4j start 2、nginx的安装及配置 nginx安装 #nginx安装 wget &#39;http://nginx.org/download/nginx-1.9.2.tar.gz&#39; tar xzvf nginx-1.9.2.tar.gz cd nginx-1.9.2 ./configure --prefix=/export/soft/nginx --with-stream make make install /export/soft/nginx/sbin/nginx -c /export/soft/nginx/conf/domains/pg.test.com nginx配置 #nginx配置 pg.test.com: #user nobody; worker_processes 1; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log</description>
    </item>
    
    <item>
      <title>TFServing model load test</title>
      <link>http://babifarm.com/post/2018/12/tensorflow_serving_model_loadtest/</link>
      <pubDate>Sat, 22 Dec 2018 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2018/12/tensorflow_serving_model_loadtest/</guid>
      <description>Tensorflow Serving model test Tensorflow model #Half Plus Two test tfserving-gpu #from :https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/docker.md mkdir -p /tmp/tfserving cd /tmp/tfserving git clone https://github.com/tensorflow/serving nvidia-docker run -p 18501:8501 \ -v /tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu:/models/half_plus_two \ -e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu &amp;amp; curl -d &#39;{&amp;quot;instances&amp;quot;: [1.0, 2.0, 5.0]}&#39; \ -X POST http://localhost:18501/v1/models/half_plus_two:predict curl -H &amp;quot;Content-Type:application/json&amp;quot; -X POST --data &#39;{&amp;quot;instances&amp;quot;: [1.0, 2.0, 5.0]}&#39; -X POST http://localhost:18501/v1/models/half_plus_two:predict gatling loadtest import io.gatling.core.Predef._ import io.gatling.core.scenario.Simulation import io.gatling.http.Predef._ import scala.concurrent.duration._ class x_tfsgpuN extends</description>
    </item>
    
    <item>
      <title>TFServing models deploy</title>
      <link>http://babifarm.com/post/2018/12/tensorflow_Serving_models_deploy/</link>
      <pubDate>Sat, 15 Dec 2018 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2018/12/tensorflow_Serving_models_deploy/</guid>
      <description>Tensorflow Serving 多模型部署 1、单模型方式 多个单模型启动： #单模型 root_path=&#39;/export/mtfs/TF&#39; docker run -p 10004:8501 -p 10005:8500 -v $root_path/corepw_model:/models/corepw_model -e MODEL_NAME=corepw_model -t tensorflow/serving &amp;amp;&amp;gt; my_log &amp;amp; docker run -p 10006:8501 -p 10007:8500 -v $root_path/brand_model:/models/brand_model -e MODEL_NAME=brand_model -t tensorflow/serving &amp;amp;&amp;gt; my_log1 &amp;amp; 访问示例： #访问例子 curl -H &amp;quot;Content-Type:application/json&amp;quot; -X POST http://localhost:10004/v1/models/corepw_model:predict -d &#39; &amp;gt; { &amp;gt; &amp;quot;instances&amp;quot;: [{ &amp;gt; &amp;quot;input_sentence_lengths&amp;quot;: 37, &amp;gt; &amp;quot;input_sentences&amp;quot;: [86, 1535, 0, 0, 0, 0, 0, 0, 0, 73, 41, 4, 0,</description>
    </item>
    
    <item>
      <title>TFServing-gpu deploy</title>
      <link>http://babifarm.com/post/2018/09/TFServing-gpu&#43;deploy/</link>
      <pubDate>Sat, 22 Sep 2018 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2018/09/TFServing-gpu&#43;deploy/</guid>
      <description>TFServing gpu 服务环境搭建 TFServing_docker-ce_nvidia-docker_nvidia-dirver_cuda_cudnn安装过程： 1、docker-ce install from:https://stackoverflow.com/questions/42981114/install-docker-ce-17-03-on-rhel7/45033117#45033117 #yum remove docker docker-common container-selinux docker-selinux docker-engine yum install -y yum-utils wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -ivh epel-release-latest-7.noarch.rpm subscription-manager repos --enable=rhel-7-server-extras-rpms yum install -y http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.55-1.el7.noarch.rpm yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce systemctl restart docker systemctl status docker.service 2、nvidia-docker install from:https://blog.csdn.net/itaacy/article/details/72628792 wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker-1.0.1-1.x86_64.rpm rpm -i /tmp/nvidia-docker*.rpm &amp;amp;&amp;amp; rm /tmp/nvidia-docker*.rpm systemctl start nvidia-docker #check nvidia-docker run --rm nvidia/cuda nvidia-smi 3、cuda install #https://developer.nvidia.com/cuda-toolkit-archive wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run sh cuda_9.0.176_384.81_linux-run #安装过程中: #Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81？ (y)es/(n)o/(q)uit:n #注意：此步选择y，其余选y或者default即可 4、cudnn install #先从官方下载cudnn，解压后进行如下操作 cp cudnn/include/cudnn.h /usr/local/cuda-9.0/include cp cudnn/lib64/libcudnn* /usr/local/cuda-9.0/lib64 chmod a+r /usr/local/cuda-9.0/include/cudnn.h /usr/local/cuda-9.0/lib64/libcudnn* 5、TFServing deploy 启动TFServing: #模型路径 /root/predictproductword nvidia-docker run -p 10001:8501 -p 10000:8500 -v /root/predictproductword:/models/predictproductword -e MODEL_NAME=predictproductword -t tensorflow/serving:latest-gpu 启动日志： 08:53:40.758380: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: predictproductword version: 1} 08:53:40.806719: I tensorflow_serving/model_servers/main.cc:327] Running ModelServer at 0.0.0.0:8500 ...</description>
    </item>
    
    <item>
      <title>tensorflow-gpu install</title>
      <link>http://babifarm.com/post/2018/01/tensorflow-gpu-install/</link>
      <pubDate>Mon, 15 Jan 2018 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2018/01/tensorflow-gpu-install/</guid>
      <description>centos7下安装tensorflow-gup cuda8、cudnn8、tensorflow-gup安装过程： 1、下载相关软件 下载cuda8.0 wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run 下载cudnn8.0-v6.0 wget http://developer2.download.nvidia.com/compute/machine-learning/cudnn/secure/v6/prod/8.0_20170427/cudnn-8.0-linux-x64-v6.0.tgz?yQ3tYwXxUZLlPzHDcsdoKYsza8ARGpYxi6k1Knhuhr-ZOKb5z4schgCD8kJF8LKCbYKRFniqc3_5rMpPPsZlGz1EBe_5GKe3KevXpgG6-Q0akSJByfXqqB0lH1wsl1c1rBJ4eZzVgX9GuGZpt5oXDMEjnwJoPGFLE7OaWBgMSpABsTdq8iuhumtn9ZV6pRByDN7JKbZD 2、安装cuda8.0 sh cuda_8.0.61_375.26_linux-run Do you accept the previously read EULA? accept/decline/quit: accept Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 387.26? (y)es/(n)o/(q)uit: y Install the CUDA 8.0 Toolkit? (y)es/(n)o/(q)uit: y Enter Toolkit Location Do you want to install a symbolic link at /usr/local/cuda? (y)es/(n)o/(q)uit: y Install the CUDA 8.0 Samples? (y)es/(n)o/(q)uit: y Enter CUDA Samples Location Installing the CUDA Toolkit in /usr/local/cuda-8.0 ...</description>
    </item>
    
    <item>
      <title>core dump</title>
      <link>http://babifarm.com/post/2017/09/core-dump/</link>
      <pubDate>Fri, 22 Sep 2017 10:06:46 -0700</pubDate>
      
      <guid>http://babifarm.com/post/2017/09/core-dump/</guid>
      <description>core dump 定位分析 core dump 日志分析 core.212160 ###1、定位core发生的位置： gdb /export/servers/model/crf/so_lib/libCRFPP.so core.212160 (gdb) bt #6 &amp;lt;signal handler called&amp;gt; #7 0x00007f269c2a34be in CRFPP::TaggerImpl::forwardbackward() () from /tmp/crfpp-java-0.57-fcac54dc-13d1-48a4-b76d-b2941f15da0d-libCRFPP.so #8 0x00007f269c2aad92 in CRFPP::TaggerImpl::parse() () from /tmp/crfpp-java-0.57-fcac54dc-13d1-48a4-b76d-b2941f15da0d-libCRFPP.so #9 0x00007f269c2a18f0 in Java_org_chasen_crfpp_CRFPPJNI_Tagger_1parse_1_1SWIG_10 () from /tmp/crfpp-java-0.57-fcac54dc-13d1-48a4-b76d-b2941f15da0d-libCRFPP.so ###2、定位dump的细节 objdump -Cd /export/servers/model/crf/so_lib/libCRFPP.so &amp;gt; tt.txt cat tt.txt|grep &amp;quot;forwardbackward&amp;quot; [root@ckespam-e31e7701-2927318961-21969 App]# cat tt.txt|grep &amp;quot;forwardbackward&amp;quot; 0000000000010848 &amp;lt;CRFPP::TaggerImpl::forwardbackward()@plt&amp;gt;: 0000000000027f40 &amp;lt;CRFPP::TaggerImpl::forwardbackward()&amp;gt;: 27f5a: 0f 84 98 01 00 00 je 280f8 &amp;lt;CRFPP::TaggerImpl::forwardbackward()+0x1b8&amp;gt; 27f7a: 7e 65 jle</description>
    </item>
    
  </channel>
</rss>
